\documentclass{eecslides}
\mode<presentation>
%\usecolortheme{BBSDark}

%%------------------
% Language and font
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

%%------------------
\usepackage{graphicx}
\usepackage{color}
\usepackage{tikz}
\usetikzlibrary{calc, shapes, backgrounds, arrows}

% --- include packages
\usepackage{subfigure}
\usepackage{multicol}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}

% --- Writing certain lines in courrier 
\usepackage{courier}

%%------------------
\DeclareRobustCommand\refmark[1]{\textsuperscript{\ref{#1}}}

%%------------------ Tune the template
\setbeamertemplate{blocks}[rounded][shadow=false]

\title[Day 3]{Markov Chain Monte Carlo (MCMC) and Model Evaluation}
\vspace{0.4cm}
\vspace{0.6cm}

%%% Begin slideshow

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\section{Intro}


\frame{
\frametitle{Ecological context}
\framesubtitle{How important is elevation in defining sugar maple distribution on mont Sutton?}


\begin{columns}[T]
\begin{column}{0.6\textwidth}

Mont Sutton

\vspace{-0.7cm}

  \begin{center}
    \scalebox{0.2}{
      \includegraphics{Image/Mont_Sutton.jpg}
    }
  \end{center}

  \vspace{-0.2cm}
  
  {\large\bf Model}
  
  \vspace{-0.5cm}
  
  {\large $$P(y=1) = \beta x + \epsilon$$}

  \vspace{-0.8cm}

 where
 \begin{itemize}
  \item[$y$] is the distribution of sugar maple
  \item[$x$] is elevation
  \textcolor{red}{\large\item[$\beta$] is the importance of elevation}
  \item[$\epsilon$] is the model residuals
 \end{itemize}
\end{column}
\begin{column}{0.5\textwidth}
\vspace{-0.5cm}
<<acerSacc,dev='pdf',fig.width = 5,fig.height = 8,echo=FALSE,eval=FALSE>>=
setwd("~/Documents/Recherche/Dataset/Inventaires herbac√©s/")
library(reshape2)
### Read Sutton data
suttonLong <- read.csv("sut_herbacee_2015.csv", header=TRUE, 
                   sep = ";")

### Format the data
suttonMelt <- melt(suttonLong[ ,c(1:2, 5:6)],
                   id.vars = c("BORX", "BORY", "ESPECE"))

suttonAll <- dcast(suttonMelt, BORX * BORY ~ ESPECE, 
                   fun.aggregate = length)
                   
### Extract species, elevation and coordinates
sutton <- suttonAll[,-c(1:2,75)]
trer <- as.matrix(sutton$TRER) ### THIS IS 
xy <- suttonAll[,1:2]
elev<-as.matrix(xy[,2])

 ### THIS IS NOT THE SUGAR MAPLE
 ### THIS IS NOT THE SUGAR MAPLE
 ### THIS IS NOT THE SUGAR MAPLE
 ### THIS IS NOT THE SUGAR MAPLE
 ### THIS IS NOT THE SUGAR MAPLE
 ### THIS IS NOT THE SUGAR MAPLE

### Draw map
par(mar=c(0,0,0,0),oma=c(3,3,3,0))
plot(xy,type="n",asp=1,axes=FALSE,xlab="",ylab="",las=1)
mtext("200 m",side = 1,outer=TRUE, cex = 1.75)
mtext("1000 m",side = 2,outer=TRUE, cex = 1.75,line=-7)
mtext("Sugar maple",side = 3,outer=TRUE, cex = 2)
points(xy[which(trer==1),],pch=15,cex=1.25)
points(xy[which(trer==0),],pch=0,cex=1.25)
@
\end{column}
\end{columns}
}

\frame{
\frametitle{Linking Frequentist and Bayesian Statistics}
\framesubtitle{How can we estimate model parameters and what does it imply?}

{\Large\bf Frequentist}

\vspace{0.1cm}

%Want to find the best model parameter(s) ($\beta$) for the data at hand (sugar maple distribution)
Want to find the best model parameter(s) for the data at hand

{\Large 
	$$\textcolor{blue}{\text{Likelihood}}\hspace{1.5cm}P(\text{Data}|\text{Model})$$
}

{\large They are interested in {\bf maximizing} the \textcolor{blue}{\text{Likelihood}}}

\vspace{0.1cm}

{\large They need data}

\vspace{0.2cm}

{\bf This can be done using}

\begin{itemize}
  \item Simulated annealing
  \item The Nelder-Mead Simplex
  \item Minimizing the sums of squares
  \item \dots
\end{itemize}
}

\frame{
\frametitle{Linking Frequentist and Bayesian Statistics}
\framesubtitle{How can we estimate model parameters and what does it imply?}

{\Large\bf Bayesian}

\vspace{0.1cm}

%Want to find how good the model parameter(s) ($\beta$) given some data (sugar maple distribution)
Want to find how good the model parameter(s) are given some data

{\Large 
	$$\textcolor{orange}{\text{Posterior}}\hspace{1.5cm}P(\text{Model}|\text{Data})$$
}

{\large They are intered in the \textcolor{orange}{\text{posterior}} distribution}

\vspace{0.1cm}

{\large They need data and prior information}

\vspace{0.2cm}

{\large\bf Recall that}

{\Large 
	$$\underbrace{P(\text{Model}|\text{Data})}_{\textcolor{orange}{Posterior}}\propto \underbrace{P(\text{Data}|\text{Model})}_\text{\textcolor{blue}{Likelihood}}\underbrace{P(\text{Model})}_\text{\textcolor{green!50!black}{Prior}}$$
}

}

\frame{
\frametitle{Bayesian Statistics}
\framesubtitle{A few words about the prior}

{\Large Definition of prior probability}

\vspace{0.1cm}

The {\bf prior probability} informes us about the probability of the model being true \textit{before} the current data is considered.

\vspace{0.5cm}

{\Large Types of priors}

\vspace{0.1cm}

{\bf\large Uninformative}

\vspace{0.1cm}

These priors are meant to bring very little information about the model

\vspace{0.1cm}

{\bf\large Informative}

\vspace{0.1cm}

These priors bring information about the model that is available

}

\frame{
\frametitle{Bayesian Statistics}
\framesubtitle{A few words about the prior}

{\Large\bf Uninformative priors}

\vspace{0.1cm}

{\bf Example:} If we have no idea of how elevation influence sugar maple

\vspace{0.1cm}

{\bf Gaussian distribution}

\vspace{0.1cm}

\begin{columns}
\begin{column}{0.5\textwidth}
\large $$\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$
\end{column}
with
\begin{column}{0.5\textwidth}
\begin{itemize}
  \item $\mu = 0$
  \item $\sigma = \text{Large say 100}$
\end{itemize}
\end{column}
\end{columns}

\vspace{0.2cm}
<<uninfoPrior, echo=FALSE, dev = "tikz",fig.width=5,fig.height=2, eval=FALSE>>=
par(mar=c(0.5,5,0.5,0.5))
couleur<-rainbow(5)
curve(dnorm, -100, 100, n = 10000, col = couleur[1], xaxt = "n",
      xlab = "", ylab = "Density", las = 1, cex.lab=1.5, lwd = 3)
curve(dnorm(x,mean = 0, sd = 2), -1000, 1000, add=TRUE,
      n=10000, col="orange", lwd = 3)
curve(dnorm(x,mean = 0, sd = 5), -1000, 1000, add=TRUE,
      n=10000,col=couleur[3], lwd = 3)
curve(dnorm(x,mean = 0, sd = 8), -1000, 1000, add=TRUE,
      n=10000,col=couleur[4], lwd = 3)
curve(dnorm(x,mean = 0, sd = 10), -1000, 1000, add=TRUE,
      n=10000,col=couleur[5], lwd = 3)
curve(dnorm(x,mean = 0, sd = 100), -1000, 1000, add=TRUE,
      n=10000, lwd = 3)
legend("topright",legend=c("$\\sigma=1$",
                           "$\\sigma=2$",
                           "$\\sigma=5$",
                           "$\\sigma=8$",
                           "$\\sigma=10$",
                           "$\\sigma=100$"),col=c(couleur[1],
                                                   "orange",
                                                   couleur[3:5],
                                                   "black"),
       lty=1, lwd=3)
@

}

\frame{
\frametitle{Bayesian Statistics}
\framesubtitle{A few words about the prior}

{\Large\bf Informative priors}

\vspace{0.1cm}

{\bf Example:} If we know that 
\begin{itemize}
  \item There are less sugar maples the higher we go
  \item The influence of elevation on sugar maple cannot be more than two folds
\end{itemize}

\vspace{0.1cm}

{\bf Uniform distribution}

\vspace{0.1cm}

\begin{columns}
\begin{column}{0.5\textwidth}
\large $$\left\{
  \begin{array}{cl}
    \frac{1}{b-a} & \text{for } x\in [a,b]\\
    0 &\text{otherwise}\\
  \end{array}
\right.$$
\end{column}
with
\begin{column}{0.5\textwidth}
\begin{itemize}
  \item $a > -2$
  \item $b < 0$
\end{itemize}
\end{column}
\end{columns}

\vspace{0.2cm}
<<infoPrior, echo=FALSE, dev = "tikz",fig.width=5,fig.height=1.5,eval=FALSE>>=
par(mar=c(2,5,0.5,0.5))
curve(dunif(x,min = -2, max = 0), n=1000, -2.5, 0.5, xlab = "",
      ylab = "Density", las = 1, cex.lab=1.5, lwd = 3)
@
}


\section{Bayesian}

\frame{
\frametitle{Bayesian Statistics}
\framesubtitle{Estimating model parameters}

{\large\bf When the likelihood \textit{can} be solved analytically}

Start from Dominique's day 2 work.

%\vspace{0.2cm}

%{\bf Example}

%\vspace{0.1cm}

%Assuming that the sugar maple follows a binomial distribution where

%\begin{itemize}[leftmargin=2cm]
%  \item[Data:] $y$
%  \item[Model ($\theta$):] $\beta x$
%\end{itemize}
%then the \textcolor{blue}{likelihood} is
%\begin{align*}
%  P(y|\theta)&=\binom{1}{y}\theta^y\left(1-\theta\right)^{1-y}\\
%  &\propto\theta^y\left(1-\theta\right)^{1-y}\\
%  &\propto\left(\beta x\right)^y\left(1-\beta x\right)^{1-y}\\
%\end{align*}


}

\frame{
\frametitle{Bayesian Statistics}
\framesubtitle{Estimating $\beta$}
Exercice (by hand?)
}

\frame{
\frametitle{Bayesian Statistics}
\framesubtitle{Estimating $\beta$}

{\Large When the likelihood {\bf cannot} be solved analytically (Difficult problem)}

Talk about MCMC
Mention that MCMC {\bf can} be used for simple problems but it is generally time consumming so generally not the best way to go
}

\section{MCMC}

\frame{
\frametitle{Properties Markov Chain Monte Carlo (MCMC)}

{\large\bf Similarity with simulated annealing}
\begin{itemize}
  \item New parameter values are chosen sequentially but randomly
  \item There are many ways to choose and accept new parameter values
\end{itemize}

\vspace{0.2cm}

{\large\bf Difference with simulated annealing}
\begin{itemize}
  \item The main goal of MCMC is to sample the \textcolor{orange}{posterior} distribution
  \item No ``temperature'' is defined
  \item It is impossible to get stuck in a loop while iterating
\end{itemize}

\vspace{0.2cm}

{\large\bf Assumptions of MCMC}

\begin{itemize}
  \item All potential parameter combinations can be reached from all other parameter combination
  \item After enough iterations the chain will converges to a stationary distribution
\end{itemize}
}

\section{Metropolis-Hasting}

\frame{
\frametitle{The Metropolis-Hasting Algorithm}
\framesubtitle{Theory}
\large
{\bf Usefulness}
\begin{itemize}
  \item It is an MCMC well designed to approach univariate problems
  \item It is useful to sample Bayesian posterior distribution
\end{itemize}

{\bf Properties}
\begin{itemize}
  \item Each iteration generates a sample from the target distribution
  \item Samples are dependent on one another (they are autocorrelated)... So effective sample size is smaller than the chain length
\end{itemize}
}


\frame{
\frametitle{The Metropolis-Hasting Algorithm}
\framesubtitle{How the Metropolis-Hasting Algorithm works}

{\Large\bf Defining the important parts}

\vspace{0.2cm}

\begin{itemize}
  \item {\large Number of steps ($N$) to run the MCMC}

  \vspace{0.2cm}

  \hspace{0.2cm} It has to be large. For example, 5000 steps

  \vspace{0.2cm}
  
  \item {\large Starting value ($\theta$)}

  \vspace{0.2cm}

  \hspace{0.2cm} It should roughly describe the distribution to be estimated 

  \vspace{0.2cm}
  
  \item {\large Target distribution ($f(\theta)$)}

  \vspace{0.2cm}

  \hspace{0.2cm} It is the distribution of value we aim at estimating
  
  \vspace{0.2cm}

  \item {\large Jumping distribution ($j(\theta_{cand}|\theta_{t-1})$)}

  \vspace{0.2cm}
  
  \hspace{0.2cm} Many choices are possible but it must allow for positive recurrence

  \vspace{0.2cm}
  
  \hspace{0.2cm} The normal distribution ($\mu$ = $\theta_{cand}$, $\sigma^2$ = 1) is a good starting point
     $$\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$
  

\end{itemize}
}


\frame{
\frametitle{The Metropolis-Hasting Algorithm}
\framesubtitle{How the Metropolis-Hasting Algorithm works}

{\Large\bf Defining the important parts (conitnued)}

\vspace{0.2cm}

\begin{itemize}
  \item {\large Acceptance probability}
  $$r=\frac{f(\theta_{cand})j(\theta_{t-1}|\theta_{cand})}{f(\theta_{t-1})j(\theta_{cand}|\theta_{t-1})}$$
\end{itemize}

\vspace{1cm}

Note that for any \textit{symmetric} distribution, such as the uniform distribution, the acceptance probability becomes
$$r=\frac{f(\theta_{cand})}{f(\theta_{t-1})}$$
This is known as the \textbf{Metropolis algorithm}
}
\frame{
\frametitle{The Metropolis-Hasting Algorithm}
\framesubtitle{How the Metropolis-Hasting Algorithm works conceptually}

\bf

{\Large Algorithm in \texttt{pseudocode}}

\vspace{0.4cm}

\texttt{for t in 1 to N}

\vspace{0.2cm}

\hspace{0.5cm}\texttt{sample theta\_cand from j(theta|theta[t-1])}
  
\vspace{0.2cm}

\hspace{0.5cm}\texttt{set r\_cand = r(theta\_cand, theta[t-1])}
  
\vspace{0.2cm}

\hspace{0.5cm}\texttt{sample U from uniform(0, 1)}
  
\vspace{0.2cm}

\hspace{0.5cm}\texttt{IF U < r}

\vspace{0.2cm}

\hspace{1cm}\texttt{theta[t] = theta\_cand}

\vspace{0.2cm}

\hspace{0.5cm}\texttt{ELSE}

\vspace{0.2cm}

\hspace{1cm}\texttt{theta[t] = theta[t-1]}

}

\section{Good Practices}

\frame{
\frametitle{Good Practices}
\framesubtitle{Chain length}

{\large\bf A rough procedure to decide how many steps is enough}

\begin{itemize}
  \item[{\bf Step 1}] Perform a pilot run for a reduced number of steps (10 to 100) and measure the time it takes
  \item[{\bf Step 2}] Decide on a number of steps to run the algorithm to obtain a result in a reasonable amount of time
  \item[{\bf Step 3}] Run the algorithm again !
  \item[{\bf Step 4}] Study the chain visually
\end{itemize}

\vspace{0.2cm}

{\large\bf The Raftery-Lewis diagnostic}

\vspace{0.2cm}

It relies on a pilot run to estimate the number of steps to be carried out

\vspace{0.1cm}

It is implemented in the \texttt{raftery.diag} function of the \texttt{coda} R package

}

\frame{
\frametitle{Good Practices}
\framesubtitle{Trace plot}

<<trace,dev='tikz',fig.width = 8,fig.height = 5,echo=FALSE,eval=FALSE>>=
par(mfrow=c(3,1),mar=c(2,2,3,0.5),oma=c(5,5,0,0))

perfect <- rnorm(5000,mean=3)
plot(perfect,type="l", ylab="",xlab="",las=1,cex.axis=2, col= "red")
title("Perfect",cex.main=3)

auto <- as.vector(arima.sim(n=5000,list(ar=c(0.95)),mean=0.1))
plot(auto,type="l", ylab="",xlab="",las=1,cex.axis=2, col= "blue")
title("Needs to be ran longer with thinning",cex.main=3)

burn <- 3/(1+(1:5000)^(-0.5))+rnorm(5000,sd=0.15)
plot(burn,type="l", ylab="",xlab="",las=1,cex.axis=2, col= "darkgreen")
title("Needs burn-in or a better starting value",cex.main=3)

mtext("Steps",side=1,cex=3,outer=TRUE,line=1.75)
mtext("$\\theta$",side=2,cex=3,outer=TRUE,line=1.25)
@
}

\frame{
\frametitle{Good Practices}
\framesubtitle{Thinning}

\large

Thinning is essentially {\bf subsampling}

\vspace{-0.5cm}

<<thining1,dev='tikz',fig.width = 8,fig.height = 2.5,echo=FALSE,eval=FALSE>>=
par(mar=c(5,5,3,0.5))

plot(auto, type = "l", xlab = "Steps",  ylab = "$\\theta$", 
     las=1, cex.axis = 1.2, cex.lab = 3, col= "blue")
@

If we ran the same MCMC as above but instead for 50000 steps and we save the $\theta$ at every 10 steps, we obtain

\vspace{-0.2cm}

<<thining2,dev='tikz',fig.width = 8,fig.height = 2.5,echo=FALSE,eval=FALSE>>=
par(mar=c(5,5,3,0.5))

autoLong <- as.vector(arima.sim(n=50000,list(ar=c(0.95)),mean=0.1))
plot(autoLong[seq(10,50000,by=10)], type = "l", xlab = "Steps", 
     ylab = "$\\theta$", las = 1, cex.axis = 1.2, cex.lab = 3, 
     col = "blue")
@

}

\frame{
\frametitle{Good Practices}
\framesubtitle{Burn-in}

\large

Burn-in is throwing away some iterations at the beginning of the MCMC run 

\vspace{-0.5cm}

<<burnin1,dev='tikz',fig.width = 8,fig.height = 2.5,echo=FALSE,eval=FALSE>>=
par(mar=c(5,5,3,0.5))

plot(burn, type = "l", xlab = "Steps",  ylab = "$\\theta$", 
     las=1, cex.axis = 1.2, cex.lab = 3, col= "darkgreen")

abline(v=500,col="red",lwd=3)
@

After burn-in, we obtain

\vspace{-0.2cm}

<<burnin2,dev='tikz',fig.width = 8,fig.height = 2.5,echo=FALSE,eval=FALSE>>=
par(mar=c(5,5,3,0.5))

burnin<-burn
burnin[1:500]<-NA

plot(burnin, type = "l", xlab = "Steps",
     ylab = "$\\theta$", las=1, cex.axis = 1.2, cex.lab = 3, 
     col= "darkgreen")
@

}

\section{Exercice 1}

\frame{
\frametitle{Exercice}
\framesubtitle{Write your own Metropolis-Hasting Algorithm}

{\Large\bf 
\begin{itemize}[leftmargin=1.75cm]
  \item[Part 1] Find the mean of the following series of values using the Metropolis-Hasting algorithm
\end{itemize}
}

\vspace{0.5cm}

\large

\begin{center}
  {\huge\texttt{2 7 9 4 3 1 3 6 1 1}}
\end{center}

\vspace{0.5cm}

The following R code was used to generate these values

<<MHex1,eval=FALSE>>=
set.seed(12)

rpois(10,5)
@

{\Large\bf 
\begin{itemize}[leftmargin=1.75cm]
  \item[Part 2] Draw a trace plot to study the result of your Metropolis-Hasting algorithm
\end{itemize}
}
}

\section{Convergence}

\frame{
\frametitle{Convergence}
\framesubtitle{Multiple chains}
\large
Rerun the estimation procedure multiple time with different starting values

<<multChain,dev='tikz',fig.width = 8,fig.height = 2.5,echo=FALSE,eval=FALSE>>=
auto2 <- as.vector(arima.sim(n=5000,list(ar=c(0.92)),mean=0.5))

par(mar=c(5,5,3,0.5))
plot(auto, type = "l", xlab = "Steps",  ylab = "$\\theta$", 
     las=1, cex.axis = 1.2, cex.lab = 3, col = "blue", 
     ylim = range(auto,auto2))
lines(auto2,col="orange",lty=2)
@
}


\frame{
\frametitle{Convergence}
\framesubtitle{Density plot}
\large

<<densPlot,dev='tikz',fig.width = 8,fig.height = 5.5,echo=FALSE,eval=FALSE>>=
par(mar=c(5,5,3,0.5))

plot(density(auto), xlab = "$\\theta", col = "blue", lwd = 3,
     main = "", cex.axis = 1.2, cex.lab = 3, las = 1)
@
}

\frame{
\frametitle{Convergence}
\framesubtitle{Geweke convergence diagnostics}
{\bf It compares two sections of the same chain}

\vspace{0.2cm}

Technically, it is two sample $t$ test of mean with unequal variance 

{\large $$Z = \frac{\bar\theta_A-\bar\theta_B}{\sqrt{\frac{S_A}{n_A}+\frac{S_B}{n_B}}}$$}

\vspace{-0.5cm}

where

\vspace{0.2cm}

$\bar\theta_A$ and $\bar\theta_B$ are the means of the chain section $\theta_A$ and $\theta_B$,

$n_A$ and $n_B$ are the number of steps of the chain section $\theta_A$ and $\theta_B$,

\vspace{0.2cm}

\begin{columns}
  \begin{column}{0.5\textwidth}
$$S_A = \frac{\sigma_A^2}{(1-\sum\alpha_A)^2}$$
  \end{column}
  \begin{column}{0.5\textwidth}
$$S_B = \frac{\sigma_B^2}{(1-\sum\alpha_B)^2}$$
  \end{column}
\end{columns}

\vspace{0.2cm}

$\sigma_A$ and $\sigma_B$ are the variance of the chains sections $\theta_A$ and $\theta_B$

$\alpha_A$ and $\alpha_B$ are autoregressive parameters of the chain sections $\theta_A$ and $\theta_B$ 

\vspace{0.5cm}

It is implemented in the \texttt{geweke.diag} function of the \texttt{coda} R package
}

\frame{
\frametitle{Convergence}
\framesubtitle{Gelman-Rubin convergence diagnostics}
\footnotesize
{\bf It compares multiple chains}

\vspace{0.2cm}

it is a corrected ratio of the pooled variance of all chains with the within variance of each chain

$$R_c=\sqrt{\frac{d+3}{d+1}\frac{V}{W}}$$

where $d$ is the degree of freedom estimate of a $t$ distribution

\vspace{0.2cm}

{\bf Chains pooled variance}
$$V=\frac{N-1}{N}W+\frac{M+1}{MN}B$$
where 
$$B=\frac{N}{M-1}\sum_{m=1}^M(\bar\theta_m-\bar\theta)^2,$$
$N$ is the length of each chain (it is assumed to be the same)

$\bar\theta_m$ is the mean chain $m$,

$\bar\theta$ is the mean of all chains.

\vspace{0.2cm}

{\bf Within chain variance}
$$W=\frac{1}{M}\sum_{m=1}^M\sigma^2$$
where $M$ is the number of chains

\vspace{0.2cm}

It is implemented in the \texttt{gelman.diag} function of the \texttt{coda} R package.
}

\end{document}

\frame{
\frametitle{How to get an interpretable estimation from an MCMC?}

{\Large\bf To make a good estimation, it is important to}

\begin{itemize}
  \item Have use enough steps (a long enought chain)
  \item Remove some of the first steps (burning)
  \item Take care of the autocorrelation related to consecutive steps (thinning)
  \item Run the algorithm multiple times (have multiple chain)
\end{itemize}
}

\end{document}

\section{Diagnostics}

\frame{
\frametitle{Convergence Diagnostics}

{\Large\bf The importance of convergence}

\vspace{0.2cm}

To obtain good quality model estimations, it is essential for the MCMC to converge properly.

\vspace{0.1cm}

But what does this mean exactly\dots

\vspace{0.2cm}

{\Large\bf Evaluating convergence}

\vspace{0.2cm}

{\large\bf Visually}

\begin{itemize}
  \item Trace plot
  \item Density plot
\end{itemize}

{\large\bf Using diagnostic statistics}

\begin{itemize}
  \item Gelman and Rubin's convergence diagnostics
  \item Geweke's convergence diagnostics
  \item Heidelberger and Welch's convergence diagnostics
  \item Raftery and Lewis's convergence diagnostics
\end{itemize}

\vspace{0.2cm}

}

\end{document}


\subsection{Adaptive Metropolis}

\frame{
\frametitle{The Adaptive Metropolis Algorithm}
Show what needs to be added to make MA adaptative

show how this helps with the example

}

\subsection{SCAM}

\frame{
\frametitle{The Single Component Adaptive Metropolis Algorithm}
Show what needs to be added to make MA a SCAM

show how this helps with the example
}

\frame{
\frametitle{The Single Component Adaptive Metropolis Algorithm with Rotation}
Show what needs to be added to make MA a SCAM

show how this helps with the example
}

\frame{
\frametitle{Gibbs Sampling}
Gibbs sampling is a special case of MA

Show an example of how this works

This is particularly useful for hierarchical models
}

\frame{
\frametitle{Metropolis-Hastings }
Exercice: Build your own for a one parameter model
}

\frame{
\frametitle{Metropolis-Hastings \& Gibbs sampler}
Exercice: Build your own for a two parameter model
}

\section{Diagnostics}
\subsection{Visual}

\frame{
\frametitle{Trace plot}

}

\frame{
\frametitle{Density plot}

}

\frame{
\frametitle{Trace plot}
Exercice: Build your own and compare it to the one of \texttt{coda}

}

\subsection{Statistics}

\frame{
\frametitle{Gelman and Rubin's convergence diagnostics}

}

\frame{
\frametitle{Geweke's convergence diagnostics}

}

\frame{
\frametitle{Heidelberger and Welch's convergence diagnostics}

}

\frame{
\frametitle{Raftery and Lewis's convergence diagnostics}

}

\section{Good Practices}

\frame{
\frametitle{Number of runs}

}
\frame{
\frametitle{Number of iterations}

}

\frame{
\frametitle{Burning}

}

\frame{
\frametitle{Thinning}

}


\end{document}
