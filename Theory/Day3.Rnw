\documentclass{eecslides}
\mode<presentation>
%\usecolortheme{BBSDark}

%%------------------
% Language and font
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

%%------------------
\usepackage{graphicx}
\usepackage{color}
\usepackage{tikz}
\usetikzlibrary{calc, shapes, backgrounds, arrows}

% --- include packages
\usepackage{subfigure}
\usepackage{multicol}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}

%%------------------
\DeclareRobustCommand\refmark[1]{\textsuperscript{\ref{#1}}}

%%------------------ Tune the template
\setbeamertemplate{blocks}[rounded][shadow=false]

\title[Day 3]{Markov Chain Monte Carlo (MCMC) and Model Evaluation}
\vspace{0.4cm}
\vspace{0.6cm}

%%% Begin slideshow

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\section{Introduction}


\frame{
\frametitle{Ecological context}
\framesubtitle{How important is elevation in defining sugar maple distribution on mont Sutton?}


\begin{columns}[T]
\begin{column}{0.6\textwidth}

Mont Sutton

\vspace{-0.7cm}

  \begin{center}
    \scalebox{0.2}{
      \includegraphics{Image/Mont_Sutton.jpg}
    }
  \end{center}

  \vspace{-0.2cm}
  
  {\large\bf Model}
  
  \vspace{-0.5cm}
  
  {\large $$P(y=1) = \beta x + \epsilon$$}

  \vspace{-0.8cm}

 where
 \begin{itemize}
  \item[$y$] is the distribution of sugar maple
  \item[$x$] is elevation
  \textcolor{red}{\large\item[$\beta$] is the importance of elevation}
  \item[$\epsilon$] is the model residuals
 \end{itemize}
\end{column}
\begin{column}{0.5\textwidth}
\vspace{-0.5cm}
<<acerSacc,dev='pdf',fig.width = 5,fig.height = 8,echo=FALSE,eval=TRUE>>=
setwd("~/Documents/Recherche/Dataset/Inventaires herbac√©s/")
library(reshape2)
### Read Sutton data
suttonLong <- read.csv("sut_herbacee_2015.csv", header=TRUE, 
                   sep = ";")

### Format the data
suttonMelt <- melt(suttonLong[ ,c(1:2, 5:6)],
                   id.vars = c("BORX", "BORY", "ESPECE"))

suttonAll <- dcast(suttonMelt, BORX * BORY ~ ESPECE, 
                   fun.aggregate = length)
                   
### Extract species, elevation and coordinates
sutton <- suttonAll[,-c(1:2,75)]
trer <- as.matrix(sutton$TRER) ### THIS IS 
xy <- suttonAll[,1:2]
elev<-as.matrix(xy[,2])

 ### THIS IS NOT THE SUGAR MAPLE
 ### THIS IS NOT THE SUGAR MAPLE
 ### THIS IS NOT THE SUGAR MAPLE
 ### THIS IS NOT THE SUGAR MAPLE
 ### THIS IS NOT THE SUGAR MAPLE
 ### THIS IS NOT THE SUGAR MAPLE

### Draw map
par(mar=c(0,0,0,0),oma=c(3,3,3,0))
plot(xy,type="n",asp=1,axes=FALSE,xlab="",ylab="",las=1)
mtext("200 m",side = 1,outer=TRUE, cex = 1.75)
mtext("1000 m",side = 2,outer=TRUE, cex = 1.75,line=-7)
mtext("Sugar maple",side = 3,outer=TRUE, cex = 2)
points(xy[which(trer==1),],pch=15,cex=1.25)
points(xy[which(trer==0),],pch=0,cex=1.25)
@
\end{column}
\end{columns}
}

\frame{
\frametitle{Linking Frequentist and Bayesian Statistics}
\framesubtitle{How can we estimate model parameters and what does it imply?}

{\Large\bf Frequentist}

\vspace{0.1cm}

%Want to find the best model parameter(s) ($\beta$) for the data at hand (sugar maple distribution)
Want to find the best model parameter(s) for the data at hand

{\Large 
	$$\textcolor{blue}{\text{Likelihood}}\hspace{1.5cm}P(\text{Data}|\text{Model})$$
}

{\large They are interested in {\bf maximizing} the \textcolor{blue}{\text{Likelihood}}}

\vspace{0.1cm}

{\large They need data}

\vspace{0.2cm}

{\bf This can be done using}

\begin{itemize}
  \item Simulated annealing
  \item The Nelder-Mead Simplex
  \item Minimizing the sums of squares
  \item \dots
\end{itemize}
}

\frame{
\frametitle{Linking Frequentist and Bayesian Statistics}
\framesubtitle{How can we estimate model parameters and what does it imply?}

{\Large\bf Bayesian}

\vspace{0.1cm}

%Want to find how good the model parameter(s) ($\beta$) given some data (sugar maple distribution)
Want to find how good the model parameter(s) are given some data

{\Large 
	$$\textcolor{orange}{\text{Posterior}}\hspace{1.5cm}P(\text{Model}|\text{Data})$$
}

{\large They are intered in the \textcolor{orange}{\text{posterior}} distribution}

\vspace{0.1cm}

{\large They need data and prior information}

\vspace{0.2cm}

{\large\bf Recall that}

{\Large 
	$$\underbrace{P(\text{Model}|\text{Data})}_{\textcolor{orange}{Posterior}}\propto \underbrace{P(\text{Data}|\text{Model})}_\text{\textcolor{blue}{Likelihood}}\underbrace{P(\text{Model})}_\text{\textcolor{green!50!black}{Prior}}$$
}

}

\frame{
\frametitle{Bayesian Statistics}
\framesubtitle{A few words about the prior}

{\Large Definition of prior probability}

\vspace{0.1cm}

The {\bf prior probability} informes us about the probability of the model being true \textit{before} the current data is considered.

\vspace{0.5cm}

{\Large Types of priors}

\vspace{0.1cm}

{\bf\large Uninformative}

\vspace{0.1cm}

These priors are meant to bring very little information about the model

\vspace{0.1cm}

{\bf\large Informative}

\vspace{0.1cm}

These priors bring information about the model that is available

}

\frame{
\frametitle{Bayesian Statistics}
\framesubtitle{A few words about the prior}

{\Large\bf Uninformative priors}

\vspace{0.1cm}

{\bf Example:} If we have no idea of how elevation influence sugar maple

\vspace{0.1cm}

{\bf Gaussian distribution}

\vspace{0.1cm}

\begin{columns}
\begin{column}{0.5\textwidth}
\large $$\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$
\end{column}
with
\begin{column}{0.5\textwidth}
\begin{itemize}
  \item $\mu = 0$
  \item $\sigma = \text{Large say 100}$
\end{itemize}
\end{column}
\end{columns}

\vspace{0.2cm}
<<uninfoPrior, echo=FALSE, dev = "tikz",fig.width=5,fig.height=2>>=
par(mar=c(0.5,5,0.5,0.5))
couleur<-rainbow(5)
curve(dnorm, -100, 100, n = 10000, col = couleur[1], xaxt = "n",
      xlab = "", ylab = "Density", las = 1, cex.lab=1.5, lwd = 3)
curve(dnorm(x,mean = 0, sd = 2), -1000, 1000, add=TRUE,
      n=10000, col="orange", lwd = 3)
curve(dnorm(x,mean = 0, sd = 5), -1000, 1000, add=TRUE,
      n=10000,col=couleur[3], lwd = 3)
curve(dnorm(x,mean = 0, sd = 8), -1000, 1000, add=TRUE,
      n=10000,col=couleur[4], lwd = 3)
curve(dnorm(x,mean = 0, sd = 10), -1000, 1000, add=TRUE,
      n=10000,col=couleur[5], lwd = 3)
curve(dnorm(x,mean = 0, sd = 100), -1000, 1000, add=TRUE,
      n=10000, lwd = 3)
legend("topright",legend=c("$\\sigma=1$",
                           "$\\sigma=2$",
                           "$\\sigma=5$",
                           "$\\sigma=8$",
                           "$\\sigma=10$",
                           "$\\sigma=100$"),col=c(couleur[1],
                                                   "orange",
                                                   couleur[3:5],
                                                   "black"),
       lty=1, lwd=3)
@

}

\frame{
\frametitle{Bayesian Statistics}
\framesubtitle{A few words about the prior}

{\Large\bf Informative priors}

\vspace{0.1cm}

{\bf Example:} If we know that 
\begin{itemize}
  \item There are less sugar maples the higher we go
  \item The influence of elevation on sugar maple cannot be more than two folds
\end{itemize}

\vspace{0.1cm}

{\bf Uniform distribution}

\vspace{0.1cm}

\begin{columns}
\begin{column}{0.5\textwidth}
\large $$\left\{
  \begin{array}{cl}
    \frac{1}{b-a} & \text{for } x\in [a,b]\\
    0 &\text{otherwise}\\
  \end{array}
\right.$$
\end{column}
with
\begin{column}{0.5\textwidth}
\begin{itemize}
  \item $a > -2$
  \item $b < 0$
\end{itemize}
\end{column}
\end{columns}

\vspace{0.2cm}
<<infoPrior, echo=FALSE, dev = "tikz",fig.width=5,fig.height=1.5>>=
par(mar=c(2,5,0.5,0.5))
curve(dunif(x,min = -2, max = 0), -2.5, 0.5, xlab = "",
      ylab = "Density", las = 1, cex.lab=1.5, lwd = 3)
@
}


\section{Bayesian}

\frame{
\frametitle{Bayesian Statistics}
\framesubtitle{Estimating model parameters}

{\large\bf When the likelihood \textit{can} be solved analytically}

Start from Dominique's day 2 work.

%\vspace{0.2cm}

%{\bf Example}

%\vspace{0.1cm}

%Assuming that the sugar maple follows a binomial distribution where

%\begin{itemize}[leftmargin=2cm]
%  \item[Data:] $y$
%  \item[Model ($\theta$):] $\beta x$
%\end{itemize}
%then the \textcolor{blue}{likelihood} is
%\begin{align*}
%  P(y|\theta)&=\binom{1}{y}\theta^y\left(1-\theta\right)^{1-y}\\
%  &\propto\theta^y\left(1-\theta\right)^{1-y}\\
%  &\propto\left(\beta x\right)^y\left(1-\beta x\right)^{1-y}\\
%\end{align*}


}

\frame{
\frametitle{Bayesian Statistics}
\framesubtitle{Estimating $\beta$}
Exercice (by hand?)
}

\frame{
\frametitle{Bayesian Statistics}
\framesubtitle{Estimating $\beta$}

{\Large When the likelihood {\bf cannot} be solved analytically (Difficult problem)}

Talk about MCMC
Mention that MCMC {\bf can} be used for simple problems but it is generally time consumming so generally not the best way to go
}

\section{MCMC}

\frame{
\frametitle{Properties Markov Chain Monte Carlo (MCMC)}

{\large\bf Similarity with simulated annealing}
\begin{itemize}
  \item New parameter values are chosen sequentially but randomly
  \item There are many ways to choose and accept new parameter values
\end{itemize}

\vspace{0.2cm}

{\large\bf Difference with simulated annealing}
\begin{itemize}
  \item The main goal of MCMC is to sample the \textcolor{orange}{posterior} distribution
  \item No ``temperature'' is defined
  \item It is impossible to get stuck in a loop while iterating
\end{itemize}

\vspace{0.2cm}

{\large\bf Assumptions of MCMC}

\begin{itemize}
  \item All potential parameter combinations can be reached from all other parameter combination
  \item After enough iterations the chain will converges to a stationary distribution
\end{itemize}
}

\section{Metropolis-Hasting}

\frame{
\frametitle{The Metropolis-Hasting Algorithm}
\framesubtitle{Theory}
Present a detailled example of the algorithm
}

\subsection{Adaptive Metropolis}

\frame{
\frametitle{The Adaptive Metropolis Algorithm}
Show what needs to be added to make MA adaptative

show how this helps with the example

}

\subsection{SCAM}

\frame{
\frametitle{The Single Component Adaptive Metropolis Algorithm}
Show what needs to be added to make MA a SCAM

show how this helps with the example
}

\frame{
\frametitle{The Single Component Adaptive Metropolis Algorithm with Rotation}
Show what needs to be added to make MA a SCAM

show how this helps with the example
}

\frame{
\frametitle{Gibbs Sampling}
Gibbs sampling is a special case of MA

Show an example of how this works

This is particularly useful for hierarchical models
}

\frame{
\frametitle{Metropolis-Hastings }
Exercice: Build your own for a one parameter model
}

\frame{
\frametitle{Metropolis-Hastings \& Gibbs sampler}
Exercice: Build your own for a two parameter model
}

\section{Diagnostics}
\subsection{Visual}

\frame{
\frametitle{Trace plot}

}

\frame{
\frametitle{Density plot}

}

\frame{
\frametitle{Trace plot}
Exercice: Build your own and compare it to the one of \texttt{coda}

}

\subsection{Statistics}

\frame{
\frametitle{Gelman and Rubin's convergence diagnostics}

}

\frame{
\frametitle{Geweke's convergence diagnostics}

}

\frame{
\frametitle{Heidelberger and Welch's convergence diagnostics}

}

\frame{
\frametitle{Raftery and Lewis's convergence diagnostics}

}

\section{Good Practices}

\frame{
\frametitle{Number of runs}

}
\frame{
\frametitle{Number of iterations}

}

\frame{
\frametitle{Burning}

}

\frame{
\frametitle{Thinning}

}


\end{document}
